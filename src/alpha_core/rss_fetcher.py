"""
å°è‚¡æ–°èæƒ…ç·’åˆ†æ - RSS æŠ“å–æ¨¡çµ„
"""

import httpx
import feedparser
from trafilatura import fetch_url, extract
from datetime import datetime
from typing import Optional, List, Dict
import time
import re


def fetch_feed(feed_url: str, timeout: int = 15) -> list:
    """æŠ“å– RSS Feed"""
    try:
        # å˜—è©¦ä½¿ç”¨ httpx
        resp = httpx.get(feed_url, timeout=timeout, follow_redirects=True)
        feed = feedparser.parse(resp.text)
        return feed.entries
    except Exception as e:
        print(f"âš ï¸ RSS Fetch Error ({feed_url}): {e}")
        # Fallback: å˜—è©¦ curl_cffi (SSL bypass)
        try:
            from curl_cffi import requests as curl_requests
            resp = curl_requests.get(feed_url, timeout=timeout, impersonate="chrome", verify=False)
            feed = feedparser.parse(resp.text)
            return feed.entries
        except Exception as e2:
            print(f"âŒ RSS Fetch Failed: {e2}")
            return []


def parse_publish_time(entry) -> str:
    """è§£æç™¼å¸ƒæ™‚é–“"""
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        return datetime(*entry.published_parsed[:6]).isoformat()
    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
        return datetime(*entry.updated_parsed[:6]).isoformat()
    else:
        return datetime.now().isoformat()


def extract_full_content(url: str, max_retries: int = 3) -> Optional[str]:
    """æå–å…¨æ–‡å…§å®¹"""
    for attempt in range(max_retries):
        try:
            downloaded = fetch_url(url)
            if downloaded:
                content = extract(downloaded, include_comments=False, include_tables=False)
                if content and len(content) > 100:
                    return content
            return None
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
            else:
                print(f"âš ï¸ Content Extract Failed ({url}): {e}")
                return None
    return None


def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡å­—"""
    if not text:
        return ""
    # ç§»é™¤å¤šé¤˜ç©ºç™½
    text = re.sub(r'\s+', ' ', text)
    # ç§»é™¤å¸¸è¦‹çš„ç¶²ç«™é›œè¨Š
    text = re.sub(r'(å»¶ä¼¸é–±è®€|ç›¸é—œæ–°è|æ¨è–¦é–±è®€|æ›´å¤šå…§å®¹).*$', '', text, flags=re.IGNORECASE)
    return text.strip()


def fetch_all_feeds(feed_list: List[tuple]) -> List[Dict]:
    """æŠ“å–æ‰€æœ‰ RSS ä¾†æºçš„æ–°è"""
    all_news = []
    
    for source_name, feed_url in feed_list:
        print(f"ğŸ“¡ Fetching: {source_name}...")
        entries = fetch_feed(feed_url)
        
        for entry in entries:
            url = entry.get('link', '')
            if not url:
                continue
            
            title = entry.get('title', '').strip()
            publish_time = parse_publish_time(entry)
            
            # å–å¾—å…¨æ–‡
            full_content = extract_full_content(url)
            if not full_content:
                # ä½¿ç”¨ RSS summary ä½œç‚ºå‚™ç”¨
                full_content = entry.get('summary', entry.get('description', ''))
            
            full_content = clean_text(full_content)
            
            # éæ¿¾å¤ªçŸ­çš„å…§å®¹
            if len(full_content) < 50:
                continue
            
            all_news.append({
                'url': url,
                'title': title,
                'source': source_name,
                'publish_time': publish_time,
                'content': full_content
            })
        
        time.sleep(1)  # ç¦®è²Œæ€§å»¶é²
    
    print(f"âœ… Total fetched: {len(all_news)} articles")
    return all_news
