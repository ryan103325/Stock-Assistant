# -*- coding: utf-8 -*-
"""
CMoney è‚¡ç¥¨åˆ†é¡çˆ¬èŸ² (æ•´åˆç‰ˆ)
åŒæ™‚çˆ¬å– Category (ç”¢æ¥­åˆ†é¡) èˆ‡ Concept (æ¦‚å¿µè‚¡)
ä½¿ç”¨ requests + BeautifulSoupï¼Œä¸éœ€è¦ Playwright
"""
import os
import csv
import time
import random
import re
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
import urllib3

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# è·¯å¾‘è¨­å®š
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))  # src/tools/crawlers
TOOLS_DIR = os.path.dirname(SCRIPT_DIR)  # src/tools
SRC_DIR = os.path.dirname(TOOLS_DIR)  # src
DATA_DIR = os.path.join(SRC_DIR, "data_core")  # src/data_core
MARKET_META_DIR = os.path.join(DATA_DIR, "market_meta")  # src/data_core/market_meta

# è¼¸å‡ºæª”æ¡ˆï¼ˆç›´æ¥æ”¾åœ¨ market_meta ç›®éŒ„ï¼‰
OUTPUT_FILE = os.path.join(MARKET_META_DIR, "cmoney_all_tags.csv")

# URLs
CATEGORY_INDEX_URL = "https://www.cmoney.tw/forum/category"
CONCEPT_INDEX_URL = "https://www.cmoney.tw/forum/concept"

# User-Agent
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
]

# è¦è·³éçš„æ¨™ç±¤æ¸…å–®ï¼ˆéç”¢æ¥­/æ¦‚å¿µé¡æ¨™ç±¤ï¼‰
SKIP_TAGS = [
    # æ’è¡Œæ¦œé¡
    "ç†±é–€æ’è¡Œ", "äººæ°£æ’è¡Œ", "è¨è«–åº¦æ’è¡Œ", "æœ¬æ—¥ç†±é–€", "æœ¬é€±ç†±é–€",
    "æ¼²å¹…æ’è¡Œ", "è·Œå¹…æ’è¡Œ", "æˆäº¤é‡æ’è¡Œ", "æŒ¯å¹…æ’è¡Œ",
    
    # æ³•äººå‹•å‘é¡
    "æ³•äººè²·è¶…", "æ³•äººè³£è¶…", "å¤–è³‡è²·è¶…", "å¤–è³‡è³£è¶…", 
    "æŠ•ä¿¡è²·è¶…", "æŠ•ä¿¡è³£è¶…", "è‡ªç‡Ÿå•†è²·è¶…", "è‡ªç‡Ÿå•†è³£è¶…",
    "ä¸‰å¤§æ³•äººè²·è¶…", "ä¸‰å¤§æ³•äººè³£è¶…",
    
    # æŠ€è¡“é¢é¡
    "è‚¡åƒ¹å‰µé«˜", "è‚¡åƒ¹å‰µä½", "çªç ´æ–°é«˜", "è·Œç ´æ–°ä½",
    "é»ƒé‡‘äº¤å‰", "æ­»äº¡äº¤å‰", "KDé»ƒé‡‘äº¤å‰", "KDæ­»äº¡äº¤å‰",
    "å‡ç·šç³¾çµ", "çªç ´å­£ç·š", "è·Œç ´å­£ç·š",
    
    # æ™‚é–“ç›¸é—œ
    "ä»Šæ—¥", "æœ¬é€±", "æœ¬æœˆ", "è¿‘æœŸ", "æœ€æ–°",
    
    # å…¶ä»–
    "é«˜æ®–åˆ©ç‡", "é«˜è‚¡æ¯", "ä½æœ¬ç›Šæ¯”", "é«˜æœ¬ç›Šæ¯”",
    "èè³‡å¢åŠ ", "èåˆ¸å¢åŠ ", "å€Ÿåˆ¸è³£å‡º",
    "æ¬Šè­‰æ¨™çš„", "å¯è½‰å‚µ", "ç‰›å¸‚", "ç†Šå¸‚"
]


def should_skip_tag(tag_name: str) -> bool:
    """
    åˆ¤æ–·æ˜¯å¦è¦è·³éé€™å€‹æ¨™ç±¤
    
    Args:
        tag_name: æ¨™ç±¤åç¨±
        
    Returns:
        True = è·³é, False = ä¿ç•™
    """
    # å®Œå…¨åŒ¹é…
    if tag_name in SKIP_TAGS:
        return True
    
    # éƒ¨åˆ†åŒ¹é…ï¼ˆåŒ…å«é—œéµå­—ï¼‰
    skip_keywords = [
        "æ’è¡Œ", "äººæ°£", "ç†±é–€", "è¨è«–åº¦",
        "è²·è¶…", "è³£è¶…", "æ³•äºº", "å¤–è³‡", "æŠ•ä¿¡", "è‡ªç‡Ÿå•†",
        "å‰µé«˜", "å‰µä½", "çªç ´", "è·Œç ´",
        "ä»Šæ—¥", "æœ¬é€±", "æœ¬æœˆ", "è¿‘æœŸ", "æœ€æ–°"
    ]
    
    for keyword in skip_keywords:
        if keyword in tag_name:
            return True
    
    # ä¿ç•™
    return False


def get_soup(url):
    """ç™¼é€è«‹æ±‚ä¸¦å›å‚³ BeautifulSoup"""
    for attempt in range(3):
        try:
            headers = {"User-Agent": random.choice(USER_AGENTS)}
            resp = requests.get(url, headers=headers, timeout=15, verify=False)
            if resp.status_code == 200:
                return BeautifulSoup(resp.text, 'html.parser')
            time.sleep(1)
        except Exception as e:
            if attempt == 2:
                print(f"  âš ï¸ {url}: {e}")
            time.sleep(1)
    return None

def extract_tags_from_index(url, tag_type):
    """å¾ç´¢å¼•é é¢æå–æ‰€æœ‰åˆ†é¡/æ¦‚å¿µé€£çµ"""
    print(f"ğŸ“¡ è¼‰å…¥ {tag_type} ç´¢å¼•: {url}")
    soup = get_soup(url)
    if not soup:
        return []
    
    tags = []
    # å°‹æ‰¾æ‰€æœ‰é€£çµ
    for a in soup.find_all('a', href=True):
        href = a['href']
        text = a.get_text(strip=True)
        
        # éæ¿¾ï¼šcategory æˆ– concept é€£çµ
        if f'/forum/{tag_type}/' in href and text:
            # æå– ID
            match = re.search(r'/forum/' + tag_type + r'/([A-Z0-9]+)', href)
            if match:
                tag_id = match.group(1)
                full_url = f"https://www.cmoney.tw/forum/{tag_type}/{tag_id}"
                tags.append({
                    "id": tag_id,
                    "name": text,
                    "url": full_url,
                    "type": tag_type
                })
    
    # å»é‡
    seen = set()
    unique_tags = []
    for t in tags:
        if t["id"] not in seen:
            seen.add(t["id"])
            unique_tags.append(t)
    
    print(f"   æ‰¾åˆ° {len(unique_tags)} å€‹ {tag_type}")
    return unique_tags

def scrape_tag_stocks(tag_info):
    """çˆ¬å–å–®ä¸€åˆ†é¡/æ¦‚å¿µçš„æˆåˆ†è‚¡"""
    soup = get_soup(tag_info["url"])
    if not soup:
        return []
    
    stocks = []
    
    # æ–¹æ³• 1: å°‹æ‰¾ table__stock é€£çµ
    for a in soup.find_all('a', class_='table__stock'):
        title = a.get('title', '')
        if title:
            parts = title.split(' ', 1)
            if len(parts) >= 2:
                code = parts[0].strip()
                name = parts[1].strip()
                if code.isdigit() and len(code) >= 4:
                    stocks.append({
                        "TagId": tag_info["id"],
                        "TagName": tag_info["name"],
                        "TagType": tag_info["type"],
                        "StockCode": code,
                        "StockName": name
                    })
    
    # æ–¹æ³• 2: å°‹æ‰¾ /forum/stock/ é€£çµ (å‚™ç”¨)
    if not stocks:
        for a in soup.find_all('a', href=True):
            href = a['href']
            if '/forum/stock/' in href:
                match = re.search(r'/forum/stock/(\d+)', href)
                if match:
                    code = match.group(1)
                    name = a.get_text(strip=True)
                    if code and name and len(code) >= 4:
                        stocks.append({
                            "TagId": tag_info["id"],
                            "TagName": tag_info["name"],
                            "TagType": tag_info["type"],
                            "StockCode": code,
                            "StockName": name
                        })
    
    return stocks

def main():
    print("ğŸš€ CMoney æ•´åˆçˆ¬èŸ²å•Ÿå‹•...")
    print("   ä¾†æº 1: category (ç”¢æ¥­åˆ†é¡)")
    print("   ä¾†æº 2: concept (æ¦‚å¿µè‚¡)")
    
    os.makedirs(MARKET_META_DIR, exist_ok=True)
    
    # 1. è¼‰å…¥ category ç´¢å¼•
    categories = extract_tags_from_index(CATEGORY_INDEX_URL, "category")
    
    # 2. è¼‰å…¥ concept ç´¢å¼•
    concepts = extract_tags_from_index(CONCEPT_INDEX_URL, "concept")
    
    # 3. åˆä½µæ‰€æœ‰æ¨™ç±¤
    all_tags = categories + concepts
    print(f"\nğŸ“Š ç¸½è¨ˆ {len(all_tags)} å€‹æ¨™ç±¤å¾…çˆ¬å–")
    
    # 4. è¼‰å…¥å·²çˆ¬å–çš„æ¨™ç±¤ (æ–·é»çºŒçˆ¬)
    existing_tags = set()
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                existing_tags.add(row.get("TagId"))
    
    remaining_tags = [t for t in all_tags if t["id"] not in existing_tags]
    print(f"   å·²å®Œæˆ {len(existing_tags)}ï¼Œå‰©é¤˜ {len(remaining_tags)}")
    
    if not remaining_tags:
        print("âœ… æ‰€æœ‰æ¨™ç±¤å·²çˆ¬å–å®Œç•¢ï¼")
        return
    
    # 5. çˆ¬å–æˆåˆ†è‚¡
    all_stocks = []
    skipped_count = 0
    
    for i, tag in enumerate(tqdm(remaining_tags, desc="çˆ¬å–ä¸­")):
        # âœ¨ æ–°å¢ï¼šéæ¿¾ç„¡æ•ˆæ¨™ç±¤
        if should_skip_tag(tag["name"]):
            skipped_count += 1
            continue
        
        stocks = scrape_tag_stocks(tag)
        all_stocks.extend(stocks)
        
        # æ¯ 20 å€‹æ¨™ç±¤å„²å­˜ä¸€æ¬¡
        if (i + 1) % 20 == 0:
            save_stocks(all_stocks, append=True)
            all_stocks = []
        
        time.sleep(random.uniform(0.3, 0.8))
    
    print(f"   å·²è·³é {skipped_count} å€‹éç”¢æ¥­æ¨™ç±¤")
    
    # 6. å„²å­˜å‰©é¤˜çµæœ
    if all_stocks:
        save_stocks(all_stocks, append=True)
    
    # 7. çµ±è¨ˆ
    print_stats()

def save_stocks(stocks, append=False):
    """å„²å­˜è‚¡ç¥¨è³‡æ–™"""
    if not stocks:
        return
    
    mode = 'a' if append and os.path.exists(OUTPUT_FILE) else 'w'
    write_header = mode == 'w'
    
    with open(OUTPUT_FILE, mode, encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=["TagId", "TagName", "TagType", "StockCode", "StockName"])
        if write_header:
            writer.writeheader()
        writer.writerows(stocks)

def print_stats():
    """å°å‡ºçµ±è¨ˆè³‡è¨Š"""
    if not os.path.exists(OUTPUT_FILE):
        return
    
    import pandas as pd
    df = pd.read_csv(OUTPUT_FILE)
    
    print(f"\nâœ… å®Œæˆï¼ç¸½è¨ˆ {len(df)} ç­†è³‡æ–™")
    print(f"   Category: {len(df[df['TagType'] == 'category'])} ç­†")
    print(f"   Concept:  {len(df[df['TagType'] == 'concept'])} ç­†")
    
    # å‰ 10 å¤§æ¨™ç±¤
    top_tags = df.groupby('TagName')['StockCode'].nunique().sort_values(ascending=False).head(10)
    print("\nğŸ“Š å‰ 10 å¤§æ¨™ç±¤:")
    for name, count in top_tags.items():
        print(f"   {name}: {count} æª”")

if __name__ == "__main__":
    main()
