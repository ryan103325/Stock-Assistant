
# ==========================================
# ğŸ­ MoneyDJ ç´°ç”¢æ¥­åˆ†é¡ çˆ¬èŸ²å·¥å…·
# ==========================================
# åŠŸèƒ½: çˆ¬å– MoneyDJã€Œç”¢æ¥­åˆ†é¡ã€é é¢ (ZH00.djhtm) çš„ã€Œç´°ç”¢æ¥­ã€æ¸…å–®èˆ‡æˆåˆ†è‚¡
#       ç›´æ¥è¼¸å‡ºæ­¸æˆ¶å¾Œçš„ CSV (ID, Tag1, Tag2...)
#
# ç›®æ¨™ URL: https://www.moneydj.com/Z/ZH/ZH00.djhtm (via ZHA index)
# è¼¸å‡º: moneydj_industries_grouped.csv
# ==========================================

import requests
from bs4 import BeautifulSoup
import time
import random
from tqdm import tqdm
import os
import urllib3
from collections import defaultdict

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ================= è¨­å®šå€ =================
BASE_URL = "https://www.moneydj.com"
INDEX_URL = "https://www.moneydj.com/Z/ZH/ZHA/ZHA.djhtm"

# æª”æ¡ˆè·¯å¾‘
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
SRC_DIR = os.path.dirname(SCRIPT_DIR)  # src
DATA_DIR = os.path.join(SRC_DIR, "data_core")  # src/data_core
GROUPED_FILE = os.path.join(DATA_DIR, "market_meta", "moneydj_industries.csv")

# User-Agent æ± 
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36"
]

def get_soup(url, verbose=False):
    max_retries = 3
    for attempt in range(max_retries):
        try:
            headers = {"User-Agent": random.choice(USER_AGENTS)}
            resp = requests.get(url, headers=headers, timeout=15, verify=False)
            resp.encoding = 'cp950' # Big5
            if resp.status_code == 200:
                return BeautifulSoup(resp.text, 'html.parser')
            time.sleep(2)
        except Exception as e:
            if verbose: print(f"âš ï¸ Error: {e}")
            time.sleep(2)
    return None

def main():
    print("ğŸš€ å•Ÿå‹• MoneyDJ ç´°ç”¢æ¥­çˆ¬èŸ² (ç›´æ¥æ­¸æˆ¶æ¨¡å¼)...")
    
    # ----------------------------------------
    # Step 1: æŠ“å–ç”¢æ¥­åˆ†é¡ç´¢å¼•
    # ----------------------------------------
    print(f"ğŸ“¡ æ­£åœ¨è«‹æ±‚ç´¢å¼•é : {INDEX_URL}")
    # ----------------------------------------
    # Step 1: æŠ“å–ç”¢æ¥­åˆ†é¡ç´¢å¼• (å«ä¸»ç”¢æ¥­åˆ¤æ–·)
    # ----------------------------------------
    print(f"ğŸ“¡ æ­£åœ¨è«‹æ±‚ç´¢å¼•é : {INDEX_URL}")
    soup_index = get_soup(INDEX_URL, verbose=True)
    if not soup_index: return
    
    category_list = []
    
    # Locate the main table(s) containing the industry list
    # Usually it is in a table with specific class or just explore all tables
    tables = soup_index.find_all('table')
    
    for table in tables:
        rows = table.find_all('tr')
        for row in rows:
            cols = row.find_all('td')
            if not cols: continue
            
            # Context: Col 0 is usually "Main Industry" (e.g. æ°´æ³¥å·¥æ¥­)
            # But sometimes it's also a link.
            # Strategy: Take Col 0 text as "Main Industry"
            main_ind = cols[0].text.strip()
            if not main_ind: continue
            if "ç”¢æ¥­" in main_ind and len(main_ind) > 10: continue # Skip weird headers
            
            # Find all relevant Detail Industry links in this row
            links = row.find_all('a', href=True)
            for a in links:
                href = a['href']
                text = a.text.strip()
                href_lower = href.lower()
                
                # Check valid link (ZH00 + a=)
                if 'zh00.djhtm' in href_lower and 'a=' in href_lower:
                    if 'Link(' in href: continue
                    if not text: continue
                    
                    if href.startswith('http'): full_url = href
                    elif href.startswith('/'): full_url = BASE_URL + href
                    else: full_url = "https://www.moneydj.com/Z/ZH/ZHA/" + href
                    
                    full_url = full_url.replace('com//', 'com/')
                    
                    # Store with Main Industry
                    category_list.append({
                        'name': text, 
                        'url': full_url,
                        'main_ind': main_ind
                    })

    # Deduplicate (Keep first occurrence to preserve Main Industry association)
    # Use dict to dedupe by URL
    unique_cats = {}
    for cat in category_list:
        if cat['url'] not in unique_cats:
            unique_cats[cat['url']] = cat
    
    category_list = list(unique_cats.values())
    print(f"âœ… æ‰¾åˆ° {len(category_list)} å€‹ç´°ç”¢æ¥­åˆ†é¡ (å«ä¸»ç”¢æ¥­æ¨™ç±¤)ã€‚")
    
    # åˆå§‹åŒ–å„²å­˜çµæ§‹ (In-Memory)
    stock_groups = defaultdict(set)
    stock_names = {}
    stock_main_ind = {}
    
    # ----------------------------------------
    # Step 2: æ·±å…¥æŠ“å–
    # ----------------------------------------
    try:
        for cat in tqdm(category_list, desc="Scraping", unit="cat"):
            cat_name = cat['name']
            cat_url = cat['url']
            main_ind = cat['main_ind']
            
            time.sleep(random.uniform(0.5, 1.5)) # Safety delay
            
            soup_cat = get_soup(cat_url)
            if not soup_cat: continue
            
            table = soup_cat.find('table', class_='t01')
            if not table: continue
            
            rows = table.find_all('tr')
            for row in rows:
                cols = row.find_all('td')
                if len(cols) < 3: continue
                
                tx0 = cols[0].text.strip()
                if "ä»£è™Ÿ" in tx0 or "åç¨±" in tx0: continue
                
                # Extract ID and Name
                # User Rule: First 4 digits are ID, rest is Name
                if len(tx0) >= 4:
                    stock_id = tx0[:4]
                    stock_name = tx0[4:].strip()
                    
                    if stock_id.isdigit():
                        # Save Industry
                        stock_groups[stock_id].add(cat_name)
                        # Save Name
                        if stock_name:
                            stock_names[stock_id] = stock_name
                        # Save Main Industry (If not set, or prefer the one that matches sub-ind?)
                        # Logic: First encounter wins, or logic to prioritize? 
                        # User example: 1101 Main=æ°´æ³¥, Sub=æ°´æ³¥. 
                        # If MainInd is not set, set it.
                        if stock_id not in stock_main_ind:
                            stock_main_ind[stock_id] = main_ind

    except KeyboardInterrupt:
        print("\nâš ï¸ ä½¿ç”¨è€…ä¸­æ–·ï¼Œæ­£åœ¨ä¿å­˜ç›®å‰é€²åº¦...")
    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
    finally:
        # Step 3: ç›´æ¥è¼¸å‡ºæ­¸æˆ¶æª”æ¡ˆ (ID, Name, MainInd, Ind1, Ind2...)
        print(f"\nğŸ’¾ æ­£åœ¨å„²å­˜æ­¸æˆ¶æª”æ¡ˆ: {GROUPED_FILE}")
        
        if not stock_groups:
             print("âš ï¸ è­¦å‘Šï¼šæ²’æœ‰æŠ“å–åˆ°ä»»ä½•è³‡æ–™ï¼Œä¸é€²è¡Œå­˜æª”ã€‚")
             return

        sorted_ids = sorted(stock_groups.keys())
        output_rows = []
        
        for sid in sorted_ids:
            # Sort tags for consistency
            industries = sorted(list(stock_groups[sid]))
            sname = stock_names.get(sid, "")
            m_ind = stock_main_ind.get(sid, "")
            
            # Format: ID, Name, MainInd, Ind1, Ind2...
            row = [sid, sname, m_ind] + industries
            output_rows.append(row)
            
        try:
            with open(GROUPED_FILE, 'w', encoding='utf-8-sig') as f:
                # User requested header row
                f.write("Code,Name,Industry,SubIndustries\n")
                for row in output_rows:
                    f.write(",".join(row) + "\n")
            print(f"âœ… æˆåŠŸå„²å­˜ {len(output_rows)} ç­†è³‡æ–™ã€‚")
        except Exception as e:
            print(f"âŒ å­˜æª”å¤±æ•—: {e}")

if __name__ == "__main__":
    main()
